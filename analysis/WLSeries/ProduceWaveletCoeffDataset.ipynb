{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Produce Wavelets coefficients Dataset\n",
    "\n",
    "- purpose to be used in Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- author Sylvie Dagoret-Campagne\n",
    "- creation date August 19th 2020\n",
    "\n",
    "Decomposition up to max = 9 levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from mpl_toolkits.axes_grid1 import host_subplot\n",
    "import mpl_toolkits.axisartist as AA\n",
    "import os,sys\n",
    "\n",
    "import matplotlib.colors as colors\n",
    "import matplotlib.cm as cmx\n",
    "\n",
    "from astropy.io import fits\n",
    "import matplotlib.colors as colors\n",
    "import matplotlib.cm as cmx\n",
    "import matplotlib.dates as mdates\n",
    "from matplotlib import gridspec\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"../../tools/atmanalyticsim\") # go to parent dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import libatmscattering as atm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pywt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to enlarge the sizes\n",
    "params = {'legend.fontsize': 'x-large',\n",
    "          'figure.figsize': (12, 8),\n",
    "         'axes.labelsize': 'x-large',\n",
    "         'axes.titlesize':'x-large',\n",
    "         'xtick.labelsize':'x-large',\n",
    "         'ytick.labelsize':'x-large'}\n",
    "plt.rcParams.update(params)\n",
    "plt.rcParams['font.size'] = 18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"setup_text_plots\" not in globals():\n",
    "    from astroML.plotting import setup_text_plots\n",
    "setup_text_plots(fontsize=18, usetex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def thresh_hard_sparse(x, k):\n",
    "    \"\"\"\n",
    "    Keep only k largest entries of x and return their indices.\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : numpy array\n",
    "        Numpy array to be thresholded\n",
    "    k : int\n",
    "        Number of largest entries in absolute value to keep\n",
    "    Notes\n",
    "    \"\"\"\n",
    "    _x = x.copy()\n",
    "    ind = np.argpartition(abs(_x), -k, axis=None)[-k:]\n",
    "    ind = np.unravel_index(ind, _x.shape)\n",
    "    ind_del = np.ones(_x.shape, dtype=bool)\n",
    "    ind_del[ind] = False\n",
    "    _x[ind_del] = 0\n",
    "    return ind, _x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_to_level(indexes,coeffs_shapes):\n",
    "    \"\"\"\n",
    "    Compute the level number corresponding to the indices\n",
    "    Parameters:\n",
    "    ----------\n",
    "    indexes : numpy array of indices\n",
    "    coeffs_shape : structure of coefficients of pywt from \n",
    "    \n",
    "    return the level number of each indexes (numpy array)\n",
    " \n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    nlevels=len(coeffs_shapes)\n",
    "    sum_nb_coeffs=np.zeros(nlevels)\n",
    "\n",
    "    # first compute cumulated threshold indexes over level\n",
    "    for ilevel in np.arange(nlevels):\n",
    "        if ilevel==0:\n",
    "            sum_nb_coeffs[ilevel]=coeffs_shapes[ilevel][0]\n",
    "        else:\n",
    "            sum_nb_coeffs[ilevel]=sum_nb_coeffs[ilevel-1]+coeffs_shapes[ilevel]['d'][0]\n",
    "            \n",
    "  \n",
    "            \n",
    "    # for each index compute the level\n",
    "    indexes_levels=np.zeros_like(indexes)\n",
    "    \n",
    "    for ilevel in np.arange(nlevels):\n",
    "       \n",
    "        if ilevel==0:\n",
    "            #indexes_levels=np.where(indexes<int(sum_nb_coeffs[ilevel]),ilevel,0)\n",
    "            sel_indexes=np.where(indexes<int(sum_nb_coeffs[ilevel]))\n",
    "            indexes_levels[sel_indexes[0]]=ilevel                     \n",
    "        else:\n",
    "            #indexes_levels=np.where(np.logical_and(indexes>=int(sum_nb_coeffs[ilevel-1]),\n",
    "            #                                       indexes<=int(sum_nb_coeffs[ilevel]))\n",
    "            #                                       ,ilevel,0)\n",
    "            sel_indexes=np.where(\n",
    "                np.logical_and(indexes>=int(sum_nb_coeffs[ilevel-1]),\n",
    "                               indexes<int(sum_nb_coeffs[ilevel])))\n",
    "            indexes_levels[sel_indexes[0]]=ilevel                          \n",
    "                                                   \n",
    "    return indexes_levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATADIR=\"../../data/atm\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "atmospheric_basename_files=os.listdir(DATADIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['lsst_atm_10year_bintab.parquet',\n",
       " 'lsst_atm_10year_01.fits',\n",
       " 'lsst_atm_10year_bigimg.fits',\n",
       " 'lsst_atm_10year_07.fits',\n",
       " 'lsst_atm_10year_06.fits',\n",
       " 'lsst_atm_10year_10.fits',\n",
       " 'lsst_atm_10year_09.fits',\n",
       " 'lsst_atm_10year_bintab.fits',\n",
       " 'lsst_atm_10year_05.fits',\n",
       " 'lsst_atm_10year_04.fits',\n",
       " 'lsst_atm_10year_bintab_small.fits',\n",
       " 'lsst_atm_10year_08.fits',\n",
       " 'lsst_atm_10year_03.fits',\n",
       " '.ipynb_checkpoints',\n",
       " 'lsst_atm_10year_02.fits']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "atmospheric_basename_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file=os.path.join(DATADIR,'lsst_atm_10year_bigimg.fits')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdu = fits.open(input_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdr=hdu[0].header\n",
    "data=hdu[0].data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SIMPLE  =                    T / conforms to FITS standard                      \n",
       "BITPIX  =                  -64 / array data type                                \n",
       "NAXIS   =                    2 / number of array dimensions                     \n",
       "NAXIS1  =                  958                                                  \n",
       "NAXIS2  =                 3651                                                  \n",
       "NBATMSIM=                 3650                                                  \n",
       "ID_NUM  =                    0                                                  \n",
       "ID_YEAR =                    1                                                  \n",
       "ID_AM   =                    2                                                  \n",
       "ID_VAOD =                    3                                                  \n",
       "ID_PWV  =                    4                                                  \n",
       "ID_O3   =                    5                                                  \n",
       "ID_CLD  =                    6                                                  \n",
       "ID_RES  =                    7                                                  \n",
       "OBSERVER= 'SDC     '                                                            \n",
       "COMMENT atm sim data challenge                                                  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hdr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "NbAtmSimul=hdr['NBATMSIM']\n",
    "idx_out_num=hdr['ID_NUM']\n",
    "idx_out_year=hdr['ID_YEAR']\n",
    "idx_out_am=hdr['ID_AM']\n",
    "idx_out_vaod=hdr['ID_VAOD']\n",
    "idx_out_pwv=hdr['ID_PWV']\n",
    "idx_out_o3=hdr['ID_O3']\n",
    "idx_out_cld=hdr['ID_CLD']\n",
    "idx_out_res=hdr['ID_RES']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "num=data[1:,idx_out_num]\n",
    "year=data[1:,idx_out_year]\n",
    "airmass=data[1:,idx_out_year]\n",
    "vaod=data[1:,idx_out_vaod] # vertical aerosol depth\n",
    "pwv=data[1:,idx_out_pwv]   # precipitable water vapor (mm)\n",
    "o3=data[1:,idx_out_o3]     # ozone\n",
    "cld=data[1:,idx_out_cld]   # clouds (not used)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract wavelength Wavelength\n",
    "wl=data[0,idx_out_res:]\n",
    "transm=data[1:,idx_out_res:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_header=data[1:,idx_out_num:idx_out_res]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "NWL=wl.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "jet = plt.get_cmap('jet')\n",
    "cNorm = colors.Normalize(vmin=0, vmax=NWL)\n",
    "scalarMap = cmx.ScalarMappable(norm=cNorm, cmap=jet)\n",
    "all_colors = scalarMap.to_rgba(np.arange(NWL), alpha=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "vaodarr=vaod[:,np.newaxis]\n",
    "pwvarr=pwv[:,np.newaxis]\n",
    "o3arr=o3[:,np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y=np.concatenate((vaodarr,pwvarr,o3arr),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "WLMINSEL=340.\n",
    "WLMAXSEL=1100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexes_selected=np.where(np.logical_and(wl>=WLMINSEL,wl<=WLMAXSEL))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need even number of bins\n",
    "if len(indexes_selected)%2:\n",
    "    indexes_selected=indexes_selected[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "760"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(indexes_selected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "wl=wl[indexes_selected]\n",
    "transm=transm[:,indexes_selected]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "WLMIN=wl[0]\n",
    "WLMAX=wl[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize wavelet\n",
    "wname = \"db1\"\n",
    "w = pywt.Wavelet(wname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Produce the list of wavelet coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop on simulation\n",
    "\n",
    "all_coeffs = []\n",
    "\n",
    "for index_atm in np.arange(NbAtmSimul):\n",
    "    #print(index_atm)\n",
    "    transm0=transm[index_atm]\n",
    "    # compute expected Rayleigh scattering\n",
    "    od=atm.RayOptDepth_adiabatic(wl, altitude=atm.altitude0, costh=1/1.2)\n",
    "    att_rayleigh=np.exp(-od)\n",
    "    \n",
    "    \n",
    "    # wavelet decomposition\n",
    "    # correction from Rayleigh\n",
    "    t=wl\n",
    "    h=-2.5*np.log10(transm0/att_rayleigh)\n",
    "    \n",
    "    # max levels\n",
    "    maxlvl=pywt.dwt_max_level(data_len=len(h), filter_len=w.dec_len)\n",
    "    nlevels=maxlvl\n",
    "    \n",
    "    # wavelet decomposition\n",
    "    coeffs = pywt.wavedec(h, w, level=nlevels,mode='zero')\n",
    "        \n",
    "    all_coeffs.append(coeffs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3650"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_coeffs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlevels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_datasets1=[]\n",
    "all_datasets2=[]\n",
    "all_datasets3=[]\n",
    "all_datasets4=[]\n",
    "all_datasets5=[]\n",
    "all_datasets6=[]\n",
    "all_datasets7=[]\n",
    "all_datasets8=[]\n",
    "all_datasets9=[]\n",
    "\n",
    "for index_atm in np.arange(NbAtmSimul):\n",
    "\n",
    "    dataset1=np.concatenate((all_coeffs[index_atm][0],all_coeffs[index_atm][1]))\n",
    "    dataset2=np.concatenate((dataset1,all_coeffs[index_atm][2]))\n",
    "    dataset3=np.concatenate((dataset2,all_coeffs[index_atm][3]))\n",
    "    dataset4=np.concatenate((dataset3,all_coeffs[index_atm][4]))\n",
    "    dataset5=np.concatenate((dataset4,all_coeffs[index_atm][5]))\n",
    "    dataset6=np.concatenate((dataset5,all_coeffs[index_atm][6]))\n",
    "    dataset7=np.concatenate((dataset6,all_coeffs[index_atm][7]))\n",
    "    dataset8=np.concatenate((dataset7,all_coeffs[index_atm][8]))\n",
    "    dataset9=np.concatenate((dataset8,all_coeffs[index_atm][9]))\n",
    "       \n",
    "    all_datasets1.append(dataset1)\n",
    "    all_datasets2.append(dataset2)\n",
    "    all_datasets3.append(dataset3)\n",
    "    all_datasets4.append(dataset4)\n",
    "    all_datasets5.append(dataset5)\n",
    "    all_datasets6.append(dataset6)\n",
    "    all_datasets7.append(dataset7)\n",
    "    all_datasets8.append(dataset8)\n",
    "    all_datasets9.append(dataset9)\n",
    "    \n",
    "    \n",
    "all_datasets1=np.array(all_datasets1)\n",
    "all_datasets2=np.array(all_datasets2)\n",
    "all_datasets3=np.array(all_datasets3)\n",
    "all_datasets4=np.array(all_datasets4)\n",
    "all_datasets5=np.array(all_datasets5)\n",
    "all_datasets6=np.array(all_datasets6)\n",
    "all_datasets7=np.array(all_datasets7)\n",
    "all_datasets8=np.array(all_datasets8)\n",
    "all_datasets9=np.array(all_datasets9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3650, 4)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_datasets1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3650, 762)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_datasets9.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_out=np.concatenate((data_header,all_datasets1),axis=1)\n",
    "primary_hdu = fits.PrimaryHDU(data_out,header=hdr)\n",
    "hdu_out = fits.HDUList([primary_hdu])\n",
    "output_file=os.path.join('lsst_atm_10year_wavelets_dataset1.fits')\n",
    "hdu_out.writeto(output_file,overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_out=np.concatenate((data_header,all_datasets2),axis=1)\n",
    "primary_hdu = fits.PrimaryHDU(data_out,header=hdr)\n",
    "hdu_out = fits.HDUList([primary_hdu])\n",
    "output_file=os.path.join('lsst_atm_10year_wavelets_dataset2.fits')\n",
    "hdu_out.writeto(output_file,overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_out=np.concatenate((data_header,all_datasets3),axis=1)\n",
    "primary_hdu = fits.PrimaryHDU(data_out,header=hdr)\n",
    "hdu_out = fits.HDUList([primary_hdu])\n",
    "output_file=os.path.join('lsst_atm_10year_wavelets_dataset3.fits')\n",
    "hdu_out.writeto(output_file,overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_out=np.concatenate((data_header,all_datasets4),axis=1)\n",
    "primary_hdu = fits.PrimaryHDU(data_out,header=hdr)\n",
    "hdu_out = fits.HDUList([primary_hdu])\n",
    "output_file=os.path.join('lsst_atm_10year_wavelets_dataset4.fits')\n",
    "hdu_out.writeto(output_file,overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_out=np.concatenate((data_header,all_datasets6),axis=1)\n",
    "primary_hdu = fits.PrimaryHDU(data_out,header=hdr)\n",
    "hdu_out = fits.HDUList([primary_hdu])\n",
    "output_file=os.path.join('lsst_atm_10year_wavelets_dataset6.fits')\n",
    "hdu_out.writeto(output_file,overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_out=np.concatenate((data_header,all_datasets7),axis=1)\n",
    "primary_hdu = fits.PrimaryHDU(data_out,header=hdr)\n",
    "hdu_out = fits.HDUList([primary_hdu])\n",
    "output_file=os.path.join('lsst_atm_10year_wavelets_dataset7.fits')\n",
    "hdu_out.writeto(output_file,overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_out=np.concatenate((data_header,all_datasets8),axis=1)\n",
    "primary_hdu = fits.PrimaryHDU(data_out,header=hdr)\n",
    "hdu_out = fits.HDUList([primary_hdu])\n",
    "output_file=os.path.join('lsst_atm_10year_wavelets_dataset8.fits')\n",
    "hdu_out.writeto(output_file,overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_out=np.concatenate((data_header,all_datasets9),axis=1)\n",
    "primary_hdu = fits.PrimaryHDU(data_out,header=hdr)\n",
    "hdu_out = fits.HDUList([primary_hdu])\n",
    "output_file=os.path.join('lsst_atm_10year_wavelets_dataset9.fits')\n",
    "hdu_out.writeto(output_file,overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r--  1 dagoret  staff    325440 Aug 19 18:26 lsst_atm_10year_wavelets_dataset1.fits\r\n",
      "-rw-r--r--  1 dagoret  staff    411840 Aug 19 18:26 lsst_atm_10year_wavelets_dataset2.fits\r\n",
      "-rw-r--r--  1 dagoret  staff    587520 Aug 19 18:27 lsst_atm_10year_wavelets_dataset3.fits\r\n",
      "-rw-r--r--  1 dagoret  staff    938880 Aug 19 18:27 lsst_atm_10year_wavelets_dataset4.fits\r\n",
      "-rw-r--r--  1 dagoret  staff   3041280 Aug 19 18:27 lsst_atm_10year_wavelets_dataset6.fits\r\n",
      "-rw-r--r--  1 dagoret  staff   5814720 Aug 19 18:27 lsst_atm_10year_wavelets_dataset7.fits\r\n",
      "-rw-r--r--  1 dagoret  staff  11364480 Aug 19 18:28 lsst_atm_10year_wavelets_dataset8.fits\r\n",
      "-rw-r--r--  1 dagoret  staff  22458240 Aug 19 18:28 lsst_atm_10year_wavelets_dataset9.fits\r\n"
     ]
    }
   ],
   "source": [
    "!ls -l *.fits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
